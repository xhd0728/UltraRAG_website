"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[896],{565(e,n,i){i.d(n,{A:()=>t});const t=i.p+"assets/images/2-2c3fd8026c9eef51e9ec81ceb3ede664.jpg"},618(e,n,i){i.d(n,{En:()=>a,Zh:()=>s});i(6540);var t=i(6666),r=i(4848);function a({children:e}){const{lang:n}=(0,t.o)();return"en"!==n?null:(0,r.jsx)(r.Fragment,{children:e})}function s({children:e}){const{lang:n}=(0,t.o)();return"zh"!==n?null:(0,r.jsx)(r.Fragment,{children:e})}},1307(e){e.exports=JSON.parse('{"permalink":"/website/blog/ultrarag-2.1-release","source":"@site/blog/2025-11-11-ultrarag-2.1-release.md","title":"UltraRAG 2.1: Deep Knowledge Integration, Cross-Modal Support","description":"In the process of building knowledge bases, setting up experimental systems, and evaluating results, researchers always encounter similar challenges: How to achieve multimodal retrieval and generation within a unified framework? How to efficiently integrate multi-source knowledge? And how to make complex RAG experiments easier to build and reproduce?","date":"2025-11-11T00:00:00.000Z","tags":[{"inline":true,"label":"release","permalink":"/website/blog/tags/release"},{"inline":true,"label":"ultrarag","permalink":"/website/blog/tags/ultrarag"}],"readingTime":9.49,"hasTruncateMarker":false,"authors":[{"name":"Sen Mei","title":"TsinghuaNLP","url":"https://mssssss123.github.io/","imageURL":"/website/img/team/ms.png","key":"ms","page":null},{"name":"Haidong Xin","title":"NEUIR","url":"https://xinhaidong.top/","imageURL":"/website/img/team/xhd.jpg","key":"xhd","page":null}],"frontMatter":{"slug":"ultrarag-2.1-release","title":"UltraRAG 2.1: Deep Knowledge Integration, Cross-Modal Support","authors":["ms","xhd"],"tags":["release","ultrarag"],"date":"2025-11-11T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"UltraRAG 3.0: No More Black Boxes, Full Transparency in Reasoning","permalink":"/website/blog/ultrarag-3.0-release"},"nextItem":{"title":"UltraRAG 2.0: Minimal Code, Maximum Innovation","permalink":"/website/blog/ultrarag-2.0-release"}}')},6362(e,n,i){i.d(n,{A:()=>t});const t=i.p+"assets/images/1-3e19c1f17e8d186267edcb7ca36225f6.jpg"},7906(e,n,i){i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>g,frontMatter:()=>l,metadata:()=>t,toc:()=>c});var t=i(1307),r=i(4848),a=i(8453),s=i(618);const l={slug:"ultrarag-2.1-release",title:"UltraRAG 2.1: Deep Knowledge Integration, Cross-Modal Support",authors:["ms","xhd"],tags:["release","ultrarag"],date:new Date("2025-11-11T00:00:00.000Z")},o=void 0,d={authorsImageUrls:[void 0,void 0]},c=[{value:"Native Multimodal Support",id:"native-multimodal-support",level:2},{value:"Automated Knowledge Integration &amp; Corpus Construction",id:"automated-knowledge-integration--corpus-construction",level:2},{value:"\u539f\u751f\u591a\u6a21\u6001\u652f\u6301",id:"\u539f\u751f\u591a\u6a21\u6001\u652f\u6301",level:2},{value:"\u77e5\u8bc6\u63a5\u5165\u4e0e\u8bed\u6599\u6784\u5efa\u81ea\u52a8\u5316",id:"\u77e5\u8bc6\u63a5\u5165\u4e0e\u8bed\u6599\u6784\u5efa\u81ea\u52a8\u5316",level:2},{value:"Unified Build &amp; Evaluate RAG Workflows",id:"unified-build--evaluate-rag-workflows",level:2},{value:"\u7edf\u4e00\u6784\u5efa\u4e0e\u8bc4\u4f30\u7684 RAG \u5de5\u4f5c\u6d41",id:"\u7edf\u4e00\u6784\u5efa\u4e0e\u8bc4\u4f30\u7684-rag-\u5de5\u4f5c\u6d41",level:2}];function u(e){const n={blockquote:"blockquote",h2:"h2",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(s.En,{children:[(0,r.jsx)(n.p,{children:"In the process of building knowledge bases, setting up experimental systems, and evaluating results, researchers always encounter similar challenges: How to achieve multimodal retrieval and generation within a unified framework? How to efficiently integrate multi-source knowledge? And how to make complex RAG experiments easier to build and reproduce?"}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"UltraRAG 2.1"})," addresses these research challenges with comprehensive upgrades focused on practical needs. This update brings core enhancements in three directions: ",(0,r.jsx)(n.strong,{children:"native multimodal support, automated knowledge integration and corpus construction, and unified build-and-evaluate RAG workflows"}),":"]}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Native Multimodal Support"}),": Unified Retriever, Generation, and Evaluation modules with full multimodal retrieval and generation support; new ",(0,r.jsx)(n.strong,{children:"VisRAG Pipeline"})," enabling a complete closed-loop from local PDF indexing to multimodal retrieval and generation."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Automated Knowledge Integration & Corpus Construction"}),": Supports multi-format document parsing and chunked indexing, seamlessly integrating MinerU for easy construction of personalized knowledge bases."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unified Build & Evaluate RAG Workflows"}),": Compatible with multiple retrieval and generation inference engines, providing a standardized evaluation system with full-chain visual analysis, achieving a unified process from model invocation to result verification."]}),"\n"]}),(0,r.jsx)(n.h2,{id:"native-multimodal-support",children:"Native Multimodal Support"}),(0,r.jsx)(n.p,{children:"Previously, multimodal RAG often relied on multiple independent tools: text tasks and visual tasks belonged to different workflows, requiring researchers to switch between feature extraction, retrieval, generation, and evaluation tools, with inconsistent interfaces and difficult reproducibility."}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"UltraRAG 2.1"})," systematically integrates the multimodal RAG pipeline. All core Servers \u2014 ",(0,r.jsx)(n.strong,{children:"Retriever, Generation, and Evaluation"})," \u2014 now natively support multimodal tasks and can flexibly connect to various visual, text, or cross-modal models. Researchers can freely orchestrate their own multimodal pipelines within the unified framework \u2014 whether for document QA, image-text retrieval, or cross-modal generation \u2014 all achievable with minimal effort for end-to-end integration. Additionally, the framework's built-in ",(0,r.jsx)(n.strong,{children:"Benchmarks"})," cover various tasks including visual QA, with a unified evaluation system for researchers to quickly conduct and compare multimodal experiments."]}),(0,r.jsxs)(n.p,{children:["Building on this, ",(0,r.jsx)(n.strong,{children:"UltraRAG 2.1 introduces the VisRAG Pipeline"}),', enabling a complete closed-loop from local PDF indexing to multimodal retrieval and generation. This feature is based on the research in "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents," which proposes a vision-enhanced retrieval-augmented generation framework for multimodal documents. By jointly modeling document image information (such as charts, formulas, layout structures) with text content, it significantly improves content understanding and QA capabilities for complex scientific documents. UltraRAG integrates this approach, enabling researchers to reproduce VisRAG experiments directly on real PDF document scenarios and further extend multimodal retrieval-generation research and applications.']}),(0,r.jsx)(n.h2,{id:"automated-knowledge-integration--corpus-construction",children:"Automated Knowledge Integration & Corpus Construction"}),(0,r.jsx)(n.p,{children:"During RAG development, developers need to repeatedly parse, clean, and chunk materials from different sources. As a result, the RAG construction process is often slowed by trivial engineering details, compressing the space for research innovation."}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"UltraRAG 2.1's"})," ",(0,r.jsx)(n.strong,{children:"Corpus Server"})," makes all of this simple. Users can import corpora from different sources in one go without writing complex scripts \u2014 whether Word documents, e-books, or web archives \u2014 all automatically parsed into a unified text format. For PDF parsing, UltraRAG seamlessly integrates ",(0,r.jsx)(n.strong,{children:"MinerU"}),", accurately recognizing complex layouts and multi-column structures for high-fidelity text restoration. For mixed image-text files, it also supports converting PDFs page-by-page to images, making visual layouts part of the knowledge. For chunking strategies, ",(0,r.jsx)(n.strong,{children:"Corpus Server"})," offers multi-granularity options: supporting token-level, sentence-level, and custom rules, enabling fine-grained control of semantic boundaries while naturally adapting to structured text like Markdown."]})]}),"\n",(0,r.jsxs)(s.Zh,{children:[(0,r.jsx)(n.p,{children:"\u5728\u7814\u7a76\u8005\u6784\u5efa\u77e5\u8bc6\u5e93\u3001\u642d\u5efa\u5b9e\u9a8c\u7cfb\u7edf\u3001\u8bc4\u4f30\u5b9e\u9a8c\u7ed3\u679c\u7684\u8fc7\u7a0b\u4e2d\uff0c\u603b\u4f1a\u9047\u5230\u76f8\u4f3c\u7684\u6311\u6218\uff1a\u5982\u4f55\u5728\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4e2d\u5b9e\u73b0\u591a\u6a21\u6001\u68c0\u7d22\u4e0e\u751f\u6210\uff1f\u5982\u4f55\u9ad8\u6548\u63a5\u5165\u591a\u6e90\u77e5\u8bc6\uff1f\u53c8\u5982\u4f55\u8ba9\u590d\u6742\u7684 RAG \u5b9e\u9a8c\u66f4\u6613\u642d\u5efa\u3001\u66f4\u6613\u590d\u73b0\uff1f"}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"UltraRAG 2.1"})," \u5728\u8fd9\u4e9b\u79d1\u7814\u6311\u6218\u7684\u80cc\u666f\u4e0b\uff0c\u8fdb\u884c\u4e86\u9762\u5411\u5b9e\u9645\u7814\u7a76\u9700\u6c42\u7684\u5168\u9762\u5347\u7ea7\u3002\u672c\u6b21\u66f4\u65b0\u56f4\u7ed5 ",(0,r.jsx)(n.strong,{children:"\u539f\u751f\u591a\u6a21\u6001\u652f\u6301\u3001\u77e5\u8bc6\u63a5\u5165\u4e0e\u8bed\u6599\u6784\u5efa\u81ea\u52a8\u5316\u3001\u7edf\u4e00\u6784\u5efa\u4e0e\u8bc4\u4f30\u7684 RAG \u5de5\u4f5c\u6d41"})," \u4e09\u5927\u65b9\u5411\u5e26\u6765\u4e86\u6838\u5fc3\u589e\u5f3a\uff1a"]}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u539f\u751f\u591a\u6a21\u6001\u652f\u6301"}),"\uff1a\u7edf\u4e00 Retriever\u3001Generation \u4e0e Evaluation \u6a21\u5757\uff0c\u5168\u9762\u652f\u6301\u591a\u6a21\u6001\u68c0\u7d22\u4e0e\u751f\u6210\uff1b\u65b0\u589e ",(0,r.jsx)(n.strong,{children:"VisRAG Pipeline"}),"\uff0c\u5b9e\u73b0\u4ece\u672c\u5730 PDF \u5efa\u5e93\u5230\u591a\u6a21\u6001\u68c0\u7d22\u4e0e\u751f\u6210\u7684\u5b8c\u6574\u95ed\u73af\u3002"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u77e5\u8bc6\u63a5\u5165\u4e0e\u8bed\u6599\u6784\u5efa\u81ea\u52a8\u5316"}),"\uff1a\u652f\u6301\u591a\u683c\u5f0f\u6587\u6863\u89e3\u6790\u4e0e\u5206\u5757\u5efa\u5e93\uff0c\u65e0\u7f1d\u96c6\u6210 MinerU\uff0c\u8f7b\u677e\u6784\u5efa\u4e2a\u4eba\u5316\u77e5\u8bc6\u5e93\u3002"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u7edf\u4e00\u6784\u5efa\u4e0e\u8bc4\u4f30\u7684 RAG \u5de5\u4f5c\u6d41"}),"\uff1a\u9002\u914d\u591a\u79cd\u68c0\u7d22\u4e0e\u751f\u6210\u63a8\u7406\u5f15\u64ce\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u652f\u6301\u5168\u94fe\u8def\u53ef\u89c6\u5316\u5206\u6790\uff0c\u5b9e\u73b0\u4ece\u6a21\u578b\u8c03\u7528\u5230\u7ed3\u679c\u9a8c\u8bc1\u7684\u7edf\u4e00\u6d41\u7a0b\u3002"]}),"\n"]}),(0,r.jsx)(n.h2,{id:"\u539f\u751f\u591a\u6a21\u6001\u652f\u6301",children:"\u539f\u751f\u591a\u6a21\u6001\u652f\u6301"}),(0,r.jsx)(n.p,{children:"\u8fc7\u53bb\uff0c\u591a\u6a21\u6001 RAG \u5f80\u5f80\u9700\u8981\u4f9d\u8d56\u591a\u5957\u72ec\u7acb\u5de5\u5177\uff1a\u6587\u672c\u4efb\u52a1\u4e0e\u89c6\u89c9\u4efb\u52a1\u5206\u5c5e\u4e0d\u540c\u6d41\u7a0b\uff0c\u7814\u7a76\u8005\u9700\u5728\u7279\u5f81\u63d0\u53d6\u3001\u68c0\u7d22\u3001\u751f\u6210\u548c\u8bc4\u4f30\u5de5\u5177\u95f4\u6765\u56de\u5207\u6362\uff0c\u63a5\u53e3\u4e0d\u7edf\u4e00\u3001\u590d\u73b0\u56f0\u96be\u3002"}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"UltraRAG 2.1"})," \u5bf9\u591a\u6a21\u6001 RAG \u6d41\u7a0b\u8fdb\u884c\u4e86\u7cfb\u7edf\u5316\u6574\u5408\u3002\u6240\u6709\u6838\u5fc3 Server\u2014\u2014",(0,r.jsx)(n.strong,{children:"Retriever\u3001Generation \u4e0e Evaluation"}),"\u2014\u2014\u5747\u5df2\u539f\u751f\u652f\u6301\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u53ef\u7075\u6d3b\u63a5\u5165\u5404\u79cd\u89c6\u89c9\u3001\u6587\u672c\u3001\u6216\u8de8\u6a21\u6001\u6a21\u578b\u3002\u7814\u7a76\u8005\u53ef\u5728\u7edf\u4e00\u6846\u67b6\u5185\u81ea\u7531\u7f16\u6392\u5c5e\u4e8e\u81ea\u5df1\u7684\u591a\u6a21\u6001 pipeline\uff0c\u65e0\u8bba\u662f\u6587\u6863\u95ee\u7b54\u3001\u56fe\u6587\u68c0\u7d22\uff0c\u8fd8\u662f\u8de8\u6a21\u6001\u751f\u6210\uff0c\u90fd\u80fd\u4ee5\u6700\u5c0f\u4ee3\u4ef7\u5b9e\u73b0\u7aef\u5230\u7aef\u8054\u901a\u3002\u6b64\u5916\uff0c\u6846\u67b6\u5185\u7f6e\u7684 ",(0,r.jsx)(n.strong,{children:"Benchmark"})," \u8986\u76d6\u89c6\u89c9\u95ee\u7b54\u7b49\u591a\u79cd\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u7edf\u4e00\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u65b9\u4fbf\u7814\u7a76\u8005\u5feb\u901f\u5f00\u5c55\u548c\u5bf9\u6bd4\u591a\u6a21\u6001\u5b9e\u9a8c\u3002"]}),(0,r.jsxs)(n.p,{children:["\u5728\u6b64\u57fa\u7840\u4e0a\uff0c",(0,r.jsx)(n.strong,{children:"UltraRAG 2.1 \u5f15\u5165 VisRAG Pipeline"}),"\uff0c\u5b9e\u73b0\u4ece\u672c\u5730 PDF \u5efa\u5e93\u5230\u591a\u6a21\u6001\u68c0\u7d22\u4e0e\u751f\u6210\u7684\u5b8c\u6574\u95ed\u73af\u3002\u8be5\u529f\u80fd\u57fa\u4e8e\u8bba\u6587\u300aVisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents\u300b\u7684\u7814\u7a76\u6210\u679c\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u591a\u6a21\u6001\u6587\u6863\u7684\u89c6\u89c9\u589e\u5f3a\u68c0\u7d22\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6587\u6863\u56fe\u50cf\u4fe1\u606f\uff08\u5982\u56fe\u8868\u3001\u516c\u5f0f\u3001\u7248\u9762\u7ed3\u6784\uff09\u4e0e\u6587\u672c\u5185\u5bb9\u8054\u5408\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u79d1\u5b66\u6587\u6863\u4e2d\u7684\u5185\u5bb9\u7406\u89e3\u4e0e\u95ee\u7b54\u80fd\u529b\u3002UltraRAG \u5c06\u8fd9\u4e00\u65b9\u6cd5\u96c6\u6210\uff0c\u4f7f\u7814\u7a76\u8005\u80fd\u591f\u76f4\u63a5\u5728\u771f\u5b9e PDF \u6587\u6863\u573a\u666f\u4e2d\u590d\u73b0 VisRAG \u7684\u5b9e\u9a8c\u8fc7\u7a0b\uff0c\u5e76\u8fdb\u4e00\u6b65\u6269\u5c55\u591a\u6a21\u6001\u68c0\u7d22\u751f\u6210\u7684\u7814\u7a76\u4e0e\u5e94\u7528\u3002"]}),(0,r.jsx)(n.h2,{id:"\u77e5\u8bc6\u63a5\u5165\u4e0e\u8bed\u6599\u6784\u5efa\u81ea\u52a8\u5316",children:"\u77e5\u8bc6\u63a5\u5165\u4e0e\u8bed\u6599\u6784\u5efa\u81ea\u52a8\u5316"}),(0,r.jsx)(n.p,{children:"\u5728 RAG \u5f00\u53d1\u8fc7\u7a0b\u4e2d\uff0c\u9762\u5bf9\u4e0d\u540c\u6765\u6e90\u7684\u8d44\u6599\uff0c\u5f00\u53d1\u8005\u9700\u8981\u53cd\u590d\u89e3\u6790\u3001\u6e05\u6d17\u3001\u5206\u5757\u3002\u7ed3\u679c\u662f\uff0cRAG \u7684\u6784\u5efa\u8fc7\u7a0b\u5f80\u5f80\u88ab\u7410\u788e\u7684\u5de5\u7a0b\u7ec6\u8282\u62d6\u6162\uff0c\u79d1\u7814\u521b\u65b0\u7684\u7a7a\u95f4\u53cd\u800c\u88ab\u538b\u7f29\u3002"}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"UltraRAG 2.1"})," \u7684 ",(0,r.jsx)(n.strong,{children:"Corpus Server"})," \u8ba9\u8fd9\u4e00\u5207\u53d8\u5f97\u7b80\u5355\u3002\u7528\u6237\u65e0\u9700\u7f16\u5199\u590d\u6742\u811a\u672c\uff0c\u5c31\u80fd\u4e00\u6b21\u6027\u5bfc\u5165\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684\u8bed\u6599\u2014\u2014\u65e0\u8bba\u662f word \u6587\u6863\u8fd8\u662f\u7535\u5b50\u4e66\u4e0e\u7f51\u9875\u5b58\u6863\uff0c\u90fd\u80fd\u88ab\u81ea\u52a8\u89e3\u6790\u4e3a\u7edf\u4e00\u7684\u6587\u672c\u683c\u5f0f\u3002\u5728 PDF \u89e3\u6790\u65b9\u9762\uff0cUltraRAG \u65e0\u7f1d\u96c6\u6210 ",(0,r.jsx)(n.strong,{children:"MinerU"}),"\uff0c\u80fd\u591f\u7cbe\u786e\u8bc6\u522b\u590d\u6742\u7248\u9762\u4e0e\u591a\u680f\u7ed3\u6784\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6587\u672c\u8fd8\u539f\u3002\u5bf9\u4e8e\u56fe\u6587\u6df7\u6392\u6587\u4ef6\uff0c\u8fd8\u652f\u6301\u5c06 PDF \u6309\u9875\u8f6c\u6362\u4e3a\u56fe\u50cf\uff0c\u8ba9\u89c6\u89c9\u5e03\u5c40\u4e5f\u80fd\u6210\u4e3a\u77e5\u8bc6\u7684\u4e00\u90e8\u5206\u3002\u5728\u5206\u5757\u7b56\u7565\u4e0a\uff0c",(0,r.jsx)(n.strong,{children:"Corpus Server"})," \u63d0\u4f9b\u4e86\u591a\u7c92\u5ea6\u9009\u62e9\uff1a\u652f\u6301\u8bcd\u5143\u7ea7\u3001\u53e5\u5b50\u7ea7\u4e0e\u81ea\u5b9a\u4e49\u89c4\u5219\uff0c\u65e2\u80fd\u7cbe\u7ec6\u63a7\u5236\u8bed\u4e49\u8fb9\u754c\uff0c\u53c8\u80fd\u81ea\u7136\u9002\u914d Markdown \u7b49\u7ed3\u6784\u5316\u6587\u672c\u3002"]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"UltraRAG 2.1 \u56fe\u793a 1",src:i(6362).A+"",width:"1280",height:"657"})}),"\n",(0,r.jsxs)(s.En,{children:[(0,r.jsx)(n.p,{children:"Through this automated pipeline, Corpus Server modularizes the corpus import, parsing, and chunking process, reducing manual scripting and format adaptation work, enabling knowledge base construction to be directly integrated into the standardized RAG pipeline workflow."}),(0,r.jsx)(n.h2,{id:"unified-build--evaluate-rag-workflows",children:"Unified Build & Evaluate RAG Workflows"}),(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:'"Chunking, indexing, retrieval, generation, evaluation \u2014 each step requires different scripts, too cumbersome!"\n"Every time I change a parameter or switch a model, do I need to rebuild the entire pipeline?"\n"After the experiment finally runs, how do I keep evaluation results consistent and comparable?"'}),"\n"]}),(0,r.jsx)(n.p,{children:"These questions are frustrations that almost every RAG researcher has experienced. Existing frameworks often provide fragmented and incompatible support for retrieval, model integration, and evaluation, forcing researchers to repeatedly switch between different tools, with every modification potentially triggering a rebuild of the entire experimental chain. UltraRAG 2.1's goal is to make complex workflows clear and unified again."}),(0,r.jsx)(n.p,{children:'At the retrieval level, the framework supports sparse, dense, hybrid, and multimodal retrieval, compatible with multiple backend engines including Infinity, Sentence-Transformers, and OpenAI. Researchers can freely combine retrieval strategies and models for flexible pipeline design. For model generation, UltraRAG 2.1 simultaneously supports vLLM offline inference and Hugging Face local debugging, while maintaining full compatibility with the OpenAI interface, making model switching and deployment require no code changes. For evaluation, UltraRAG builds a unified Evaluation Server that can compute metrics like ACC and ROUGE for generated results, and supports TREC evaluation and significance analysis for retrieval results. Combined with the visual Case Study UI, researchers can intuitively compare the performance of different models and strategies, making "debugging" truly become "understanding."'}),(0,r.jsx)(n.p,{children:"Furthermore, UltraRAG achieves full-chain integration from data import to retrieval, generation, and evaluation through a YAML configuration-driven workflow mechanism. Researchers only need to write minimal configuration files to quickly define and reproduce experimental workflows."})]}),"\n",(0,r.jsxs)(s.Zh,{children:[(0,r.jsx)(n.p,{children:"\u901a\u8fc7\u8fd9\u4e00\u6574\u5957\u81ea\u52a8\u5316\u6d41\u7a0b\uff0cCorpus Server \u5c06\u8bed\u6599\u5bfc\u5165\u3001\u89e3\u6790\u4e0e\u5206\u5757\u8fc7\u7a0b\u6a21\u5757\u5316\uff0c\u51cf\u5c11\u4e86\u624b\u5de5\u811a\u672c\u4e0e\u683c\u5f0f\u9002\u914d\u5de5\u4f5c\uff0c\u4f7f\u77e5\u8bc6\u5e93\u6784\u5efa\u53ef\u4ee5\u76f4\u63a5\u878d\u5165 RAG pipeline \u7684\u6807\u51c6\u5316\u6d41\u7a0b\u4e2d\u3002"}),(0,r.jsx)(n.h2,{id:"\u7edf\u4e00\u6784\u5efa\u4e0e\u8bc4\u4f30\u7684-rag-\u5de5\u4f5c\u6d41",children:"\u7edf\u4e00\u6784\u5efa\u4e0e\u8bc4\u4f30\u7684 RAG \u5de5\u4f5c\u6d41"}),(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:'"\u5207\u5757\u3001\u7d22\u5f15\u3001\u68c0\u7d22\u3001\u751f\u6210\u3001\u8bc4\u4f30\uff0c\u6bcf\u4e00\u6b65\u90fd\u8981\u7528\u4e0d\u540c\u7684\u811a\u672c\uff0c\u592a\u7e41\u7410\u4e86\uff01"\n"\u6bcf\u6539\u4e00\u6b21\u53c2\u6570\u3001\u6362\u4e00\u4e2a\u6a21\u578b\uff0c\u662f\u5426\u53c8\u8981\u91cd\u642d\u6574\u6761 pipeline\uff1f"\n"\u5f53\u5b9e\u9a8c\u7ec8\u4e8e\u8dd1\u901a\u540e\uff0c\u8bc4\u4f30\u7ed3\u679c\u53c8\u8be5\u600e\u6837\u4fdd\u6301\u4e00\u81f4\u4e0e\u53ef\u6bd4\uff1f"'}),"\n"]}),(0,r.jsx)(n.p,{children:"\u8fd9\u4e9b\u95ee\u9898\u51e0\u4e4e\u662f\u6bcf\u4e2a RAG \u7814\u7a76\u8005\u90fd\u7ecf\u5386\u8fc7\u7684\u70e6\u607c\u3002\u73b0\u6709\u6846\u67b6\u5bf9\u68c0\u7d22\u3001\u6a21\u578b\u63a5\u5165\u3001\u8bc4\u4f30\u7684\u652f\u6301\u5f80\u5f80\u96f6\u6563\u4e14\u4e0d\u517c\u5bb9\uff0c\u7814\u7a76\u8005\u4e0d\u5f97\u4e0d\u5728\u4e0d\u540c\u5de5\u5177\u4e4b\u95f4\u53cd\u590d\u5207\u6362\uff0c\u6bcf\u4e00\u6b21\u4fee\u6539\u90fd\u53ef\u80fd\u5f15\u53d1\u6574\u6761\u5b9e\u9a8c\u94fe\u8def\u7684\u91cd\u5efa\u3002UltraRAG 2.1 \u7684\u76ee\u6807\uff0c\u5c31\u662f\u8ba9\u590d\u6742\u7684\u6d41\u7a0b\u91cd\u65b0\u53d8\u5f97\u6e05\u6670\u800c\u7edf\u4e00\u3002"}),(0,r.jsx)(n.p,{children:'\u5728\u68c0\u7d22\u5c42\u9762\uff0c\u6846\u67b6\u652f\u6301\u7a00\u758f\u3001\u7a20\u5bc6\u3001\u6df7\u5408\u4e0e\u591a\u6a21\u6001\u68c0\u7d22\uff0c\u5e76\u517c\u5bb9 Infinity\u3001Sentence-Transformers\u3001OpenAI \u7b49\u591a\u79cd\u540e\u7aef\u5f15\u64ce\uff0c\u7814\u7a76\u8005\u53ef\u4ee5\u81ea\u7531\u7ec4\u5408\u68c0\u7d22\u7b56\u7565\u4e0e\u6a21\u578b\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684 pipeline \u8bbe\u8ba1\u3002\u5728\u6a21\u578b\u751f\u6210\u90e8\u5206\uff0cUltraRAG 2.1 \u540c\u65f6\u652f\u6301 vLLM \u79bb\u7ebf\u63a8\u7406 \u4e0e Hugging Face \u672c\u5730\u8c03\u8bd5\uff0c\u5e76\u4fdd\u6301\u4e0e OpenAI \u63a5\u53e3 \u5b8c\u5168\u517c\u5bb9\uff0c\u4f7f\u6a21\u578b\u5207\u6362\u4e0e\u90e8\u7f72\u65e0\u9700\u4fee\u6539\u4ee3\u7801\u3002\u5728\u8bc4\u4f30\u73af\u8282\uff0cUltraRAG \u6784\u5efa\u4e86\u7edf\u4e00\u7684 Evaluation Server\uff0c\u65e2\u80fd\u5bf9\u751f\u6210\u7ed3\u679c\u8ba1\u7b97 ACC\u3001ROUGE \u7b49\u6307\u6807\uff0c\u53c8\u652f\u6301\u5bf9\u68c0\u7d22\u7ed3\u679c\u8fdb\u884c TREC \u8bc4\u4f30\u4e0e\u663e\u8457\u6027\u5206\u6790\u3002\u914d\u5408\u53ef\u89c6\u5316\u7684 Case Study UI\uff0c\u7814\u7a76\u8005\u53ef\u4ee5\u76f4\u89c2\u5730\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u4e0e\u7b56\u7565\u7684\u8868\u73b0\uff0c\u8ba9"\u8c03\u8bd5"\u771f\u6b63\u53d8\u6210"\u7406\u89e3"\u3002'}),(0,r.jsx)(n.p,{children:"\u6b64\u5916\uff0cUltraRAG \u901a\u8fc7 YAML \u914d\u7f6e\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u4ece\u6570\u636e\u5bfc\u5165\u5230\u68c0\u7d22\u3001\u751f\u6210\u4e0e\u8bc4\u4f30\u7684\u5168\u94fe\u8def\u4e32\u8054\uff0c\u7814\u7a76\u8005\u53ea\u9700\u7f16\u5199\u5c11\u91cf\u914d\u7f6e\u6587\u4ef6\uff0c\u5373\u53ef\u5feb\u901f\u5b9a\u4e49\u548c\u590d\u73b0\u5b9e\u9a8c\u6d41\u7a0b\u3002"})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"UltraRAG 2.1 \u56fe\u793a 2",src:i(565).A+"",width:"1280",height:"791"})})]})}function g(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>l});var t=i(6540);const r={},a=t.createContext(r);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);