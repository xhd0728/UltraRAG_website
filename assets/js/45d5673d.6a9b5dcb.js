"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[295],{2760(e,a,t){t.r(a),t.d(a,{default:()=>le});t(6540);var s=t(8774),n=(t(6025),t(8873)),r=t(7083);const i="heroSection_Z9Mr",l="heroContent_qtjE",o="heroTitle_vU1X",d="heroSubtitle_kRhI",c="container_WYko",h="sectionHeader_Y0cu",m="sectionTitle_SiJ3",g="sectionDesc_akIc",u="blogSection_lCNf",p="sectionHeaderRow_W0pW",f="viewAllLink_Fu35",b="blogFeatured_KVNL",x="blogFeaturedTag_ASbk",v="blogFeaturedTitle_ANJ8",R="blogFeaturedSummary_jf4A",A="blogFeaturedMeta_IwN9",j="blogMetaDivider_FxyP",N="blogGrid_fbaM",G="blogCard_azRs",_="blogCardTag_GYkW",M="blogCardTitle_ZsqN",C="blogCardSummary_Y5af",w="blogCardMeta_Mbwr",y="modelsSection_NIqE",k="modelsGrid_fS_7",S="modelCard_P1Gx",L="modelCardInner_vg_f",P="modelName_bhxY",D="modelDesc_coF9",U="modelTags_ZuK8",T="modelTag_aW0T",E="modelLink_Zx0Q",W="modelArrow_Li7v",F="papersSection_BzfN",I="papersList_fyoz",Y="paperCard_xbJw",z="paperMeta_ylCd",X="paperDate_s3j4",H="paperVenue_bswl",V="paperTitle_MgHI",Z="paperAuthors_kCxq",K="paperAbstract_u2On",q="paperLinkText_yiu8",B="paperArrow_Xwda";var O=t(4848);const J=[{title:"UltraRAG 3.0: No More Black Boxes, Full Transparency in Reasoning",summary:'Addressing the pain point of "algorithm prototyping takes a week, but building a usable system takes months", UltraRAG 3.0 brings three core upgrades: full-chain visible reasoning, modular MCP architecture, and unified evaluation system.',date:"2026.01.23",authors:"Sen Mei, Haidong Xin",tags:["Release","UltraRAG"],to:"/blog/ultrarag-3.0-release"},{title:"UltraRAG 2.1: Deep Knowledge Integration, Cross-Modal Support",summary:"Comprehensive upgrades focused on native multimodal support, automated knowledge integration and corpus construction, and unified RAG workflows for building and evaluation.",date:"2025.11.11",authors:"Sen Mei, Haidong Xin",tags:["Release","UltraRAG"],to:"/blog/ultrarag-2.1-release"},{title:"UltraRAG 2.0: Minimal Code, Maximum Innovation",summary:"The first RAG framework designed with Model Context Protocol (MCP) architecture, enabling researchers to implement multi-stage reasoning systems with just YAML files.",date:"2025.08.28",authors:"Sen Mei, Haidong Xin, Chunyi Peng",tags:["Release","UltraRAG"],to:"/blog/ultrarag-2.0-release"}],Q=[{title:"UltraRAG 3.0\uff1a\u544a\u522b\u9ed1\u76d2\uff0c\u63a8\u7406\u903b\u8f91\u5168\u900f\u660e",summary:'\u9488\u5bf9"\u9a8c\u8bc1\u7b97\u6cd5\u539f\u578b\u53ea\u9700\u4e00\u5468\uff0c\u6784\u5efa\u53ef\u7528\u7cfb\u7edf\u5374\u8017\u65f6\u6570\u6708"\u7684\u75db\u70b9\uff0cUltraRAG 3.0 \u5e26\u6765\u4e86\u5168\u94fe\u8def\u53ef\u89c6\u5316\u63a8\u7406\u3001\u6a21\u5757\u5316 MCP \u67b6\u6784\u4e0e\u7edf\u4e00\u8bc4\u6d4b\u4f53\u7cfb\u4e09\u5927\u6838\u5fc3\u5347\u7ea7\u3002',date:"2026.01.23",authors:"Sen Mei, Haidong Xin",tags:["Release","UltraRAG"],to:"/blog/ultrarag-3.0-release"},{title:"UltraRAG 2.1\uff1a\u7eb5\u6df1\u77e5\u8bc6\u63a5\u5165\uff0c\u6a2a\u8de8\u591a\u6a21\u6001\u652f\u6301",summary:"\u56f4\u7ed5\u539f\u751f\u591a\u6a21\u6001\u652f\u6301\u3001\u77e5\u8bc6\u63a5\u5165\u4e0e\u8bed\u6599\u6784\u5efa\u81ea\u52a8\u5316\u3001\u7edf\u4e00\u6784\u5efa\u4e0e\u8bc4\u4f30\u7684 RAG \u5de5\u4f5c\u6d41\u4e09\u5927\u65b9\u5411\uff0c\u8fdb\u884c\u4e86\u9762\u5411\u5b9e\u9645\u7814\u7a76\u9700\u6c42\u7684\u5168\u9762\u5347\u7ea7\u3002",date:"2025.11.11",authors:"Sen Mei, Haidong Xin",tags:["Release","UltraRAG"],to:"/blog/ultrarag-2.1-release"},{title:"UltraRAG 2.0\uff1a\u4ee3\u7801\u6781\u7b80\u5316\uff0c\u521b\u65b0\u6700\u5927\u5316",summary:"\u9996\u4e2a\u57fa\u4e8e Model Context Protocol (MCP) \u67b6\u6784\u8bbe\u8ba1\u7684 RAG \u6846\u67b6\uff0c\u8ba9\u79d1\u7814\u4eba\u5458\u53ea\u9700\u7f16\u5199 YAML \u6587\u4ef6\u5373\u53ef\u5b9e\u73b0\u591a\u9636\u6bb5\u63a8\u7406\u7cfb\u7edf\u3002",date:"2025.08.28",authors:"Sen Mei, Haidong Xin, Chunyi Peng",tags:["Release","UltraRAG"],to:"/blog/ultrarag-2.0-release"}],$=[{name:"AgentCPM-Report",desc:"An intelligent Agent model for long document generation and report writing, enabling automated research report generation.",tags:["Agent","Long-Text Generation","DeepResearch"],href:"https://huggingface.co/openbmb/AgentCPM-Report"},{name:"MiniCPM-Embedding-Light",desc:"A lightweight and efficient text embedding model, achieving leading performance on multiple retrieval benchmarks, suitable for large-scale semantic retrieval in RAG scenarios.",tags:["Embedding","Retrieval"],href:"https://huggingface.co/openbmb/MiniCPM-Embedding-Light"}],ee=[{name:"AgentCPM-Report",desc:"\u9762\u5411\u957f\u6587\u6863\u751f\u6210\u4e0e\u62a5\u544a\u64b0\u5199\u7684\u667a\u80fd Agent \u6a21\u578b\uff0c\u52a9\u529b\u81ea\u52a8\u5316\u7814\u7a76\u62a5\u544a\u751f\u6210\u3002",tags:["Agent","Long-Text Generation","DeepResearch"],href:"https://huggingface.co/openbmb/AgentCPM-Report"},{name:"MiniCPM-Embedding-Light",desc:"\u8f7b\u91cf\u9ad8\u6548\u7684\u6587\u672c\u5411\u91cf\u6a21\u578b\uff0c\u5728\u591a\u9879\u68c0\u7d22\u57fa\u51c6\u4e0a\u53d6\u5f97\u9886\u5148\u8868\u73b0\uff0c\u9002\u914d RAG \u573a\u666f\u4e0b\u7684\u5927\u89c4\u6a21\u8bed\u4e49\u68c0\u7d22\u9700\u6c42\u3002",tags:["Embedding","Retrieval"],href:"https://huggingface.co/openbmb/MiniCPM-Embedding-Light"}],ae=[{title:"VisRAG: Vision-based Retrieval-Augmented Generation on Multi-modality Documents",authors:"Shi Yu, Chaoyue Tang, et al.",date:"2025.04",venue:"ICLR 2025",abstract:'Proposes a "vision-first" retrieval-augmented generation paradigm that fundamentally solves the information degradation problem of complex layout documents in traditional text parsing by converting documents directly into visual vectors for matching and generation.',href:"https://arxiv.org/pdf/2410.10594"},{title:"RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",authors:"Xinze Li, Sen Mei, et al.",date:"2025.04",venue:"ICLR 2025",abstract:'Proposes a new RAG optimization paradigm based on "differentiable data rewards", significantly improving the model\'s ability to extract core information from external knowledge and resolve knowledge conflicts through end-to-end reward alignment between retriever and generator.',href:"https://arxiv.org/pdf/2410.13509"},{title:"RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework",authors:"Kunlun Zhu, Yifan Luo, et al.",date:"2025.07",venue:"ACL 2025",abstract:"Proposes a new paradigm for automated RAG evaluation benchmark construction, enabling efficient customization of evaluation datasets for specific vertical scenarios (such as finance, law, healthcare) through Schema-based knowledge distillation and document generation.",href:"https://aclanthology.org/2025.acl-long.418.pdf"},{title:"DeepNote: Note-Centric Deep Retrieval-Augmented Generation",authors:"Ruobing Wang, et al.",date:"2025.11",venue:"EMNLP 2025",abstract:'Proposes a "note-centric" adaptive retrieval-augmented generation paradigm that significantly improves the model\'s depth and robustness in handling complex open-domain QA tasks by introducing an iterative knowledge accumulation mechanism.',href:"https://arxiv.org/pdf/2410.08821"}],te=[{title:"VisRAG: Vision-based Retrieval-Augmented Generation on Multi-modality Documents",authors:"Shi Yu, Chaoyue Tang, et al.",date:"2025.04",venue:"ICLR 2025",abstract:'\u63d0\u51fa\u4e86\u4e00\u79cd"\u89c6\u89c9\u4f18\u5148"\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u6587\u6863\u76f4\u63a5\u8f6c\u5316\u4e3a\u89c6\u89c9\u5411\u91cf\u8fdb\u884c\u5339\u914d\u4e0e\u751f\u6210\uff0c\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u4e86\u590d\u6742\u6392\u7248\u6587\u6863\u5728\u4f20\u7edf\u6587\u672c\u89e3\u6790\u4e2d\u7684\u4fe1\u606f\u964d\u7ea7\u95ee\u9898\u3002',href:"https://arxiv.org/pdf/2410.10594"},{title:"RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",authors:"Xinze Li, Sen Mei, et al.",date:"2025.04",venue:"ICLR 2025",abstract:'\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e"\u53ef\u5fae\u5206\u6570\u636e\u5956\u52b1"\u7684 RAG \u4f18\u5316\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u68c0\u7d22\u5668\u4e0e\u751f\u6210\u5668\u4e4b\u95f4\u5efa\u7acb\u7aef\u5230\u7aef\u7684\u5956\u52b1\u5bf9\u9f50\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u4ece\u5916\u90e8\u77e5\u8bc6\u4e2d\u63d0\u53d6\u6838\u5fc3\u4fe1\u606f\u5e76\u89e3\u51b3\u77e5\u8bc6\u51b2\u7a81\u7684\u80fd\u529b\u3002',href:"https://arxiv.org/pdf/2410.13509"},{title:"RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework",authors:"Kunlun Zhu, Yifan Luo, et al.",date:"2025.07",venue:"ACL 2025",abstract:"\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u8bc4\u4ef7\u57fa\u51c6\u6784\u5efa\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u57fa\u4e8e Schema \u7684\u77e5\u8bc6\u84b8\u998f\u4e0e\u6587\u6863\u751f\u6210\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9488\u5bf9\u7279\u5b9a\u5782\u76f4\u573a\u666f\uff08\u5982\u91d1\u878d\u3001\u6cd5\u5f8b\u3001\u533b\u7597\uff09\u8bc4\u4ef7\u6570\u636e\u96c6\u7684\u9ad8\u6548\u5b9a\u5236\u3002",href:"https://aclanthology.org/2025.acl-long.418.pdf"},{title:"DeepNote: Note-Centric Deep Retrieval-Augmented Generation",authors:"Ruobing Wang, et al.",date:"2025.11",venue:"EMNLP 2025",abstract:'\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5"\u7b14\u8bb0"\u4e3a\u6838\u5fc3\u7684\u81ea\u9002\u5e94\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u5f15\u5165\u8fed\u4ee3\u5f0f\u77e5\u8bc6\u79ef\u7d2f\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5904\u7406\u590d\u6742\u5f00\u653e\u57df\u95ee\u7b54\u4efb\u52a1\u7684\u6df1\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002',href:"https://arxiv.org/pdf/2410.08821"}];function se(){const e=(0,r.W)();return(0,O.jsx)("section",{className:i,children:(0,O.jsxs)("div",{className:l,children:[(0,O.jsx)("h1",{className:o,children:e?"\u7814\u7a76":"Research"}),(0,O.jsx)("p",{className:d,children:e?"\u4e86\u89e3\u6211\u4eec\u7684\u6700\u65b0\u7814\u7a76\u6210\u679c":"Discover our latest research"})]})})}function ne(){const e=(0,r.W)(),a=e?Q:J;return(0,O.jsx)("section",{id:"blog",className:u,children:(0,O.jsxs)("div",{className:c,children:[(0,O.jsxs)("div",{className:p,children:[(0,O.jsxs)("div",{className:h,children:[(0,O.jsx)("h2",{className:m,children:e?"\u535a\u5ba2":"Blog"}),(0,O.jsx)("p",{className:g,children:e?"\u8ddf\u8e2a UltraRAG \u5f00\u6e90\u8fdb\u5c55\u4e0e\u6280\u672f\u66f4\u65b0\u3002":"Follow UltraRAG open-source progress and technical updates."})]}),(0,O.jsxs)(s.A,{className:f,to:"/blog",children:[e?"\u67e5\u770b\u5168\u90e8":"View All"," ",(0,O.jsx)("span",{children:"\u2192"})]})]}),(0,O.jsxs)(s.A,{to:a[0].to,className:b,children:[(0,O.jsx)("div",{className:x,children:a[0].tags[0]}),(0,O.jsx)("h3",{className:v,children:a[0].title}),(0,O.jsx)("p",{className:R,children:a[0].summary}),(0,O.jsxs)("div",{className:A,children:[(0,O.jsx)("span",{children:a[0].date}),(0,O.jsx)("span",{className:j,children:"\xb7"}),(0,O.jsx)("span",{children:a[0].authors})]})]}),(0,O.jsx)("div",{className:N,children:a.slice(1).map((e,a)=>(0,O.jsxs)(s.A,{to:e.to,className:G,children:[(0,O.jsx)("div",{className:_,children:e.tags[0]}),(0,O.jsx)("h3",{className:M,children:e.title}),(0,O.jsx)("p",{className:C,children:e.summary}),(0,O.jsxs)("div",{className:w,children:[(0,O.jsx)("span",{children:e.date}),(0,O.jsx)("span",{className:j,children:"\xb7"}),(0,O.jsx)("span",{children:e.authors})]})]},a))})]})})}function re(){const e=(0,r.W)(),a=e?ee:$;return(0,O.jsx)("section",{className:y,children:(0,O.jsxs)("div",{className:c,children:[(0,O.jsxs)("div",{className:h,children:[(0,O.jsx)("h2",{className:m,children:e?"\u6a21\u578b":"Models"}),(0,O.jsx)("p",{className:g,children:e?"\u6211\u4eec\u5f00\u6e90\u7684\u6838\u5fc3\u6a21\u578b\uff0c\u4e3a RAG \u751f\u6001\u63d0\u4f9b\u57fa\u7840\u80fd\u529b\u652f\u6491\u3002":"Our open-source core models, providing foundational capabilities for the RAG ecosystem."})]}),(0,O.jsx)("div",{className:k,children:a.map((a,t)=>(0,O.jsx)("a",{href:a.href,target:"_blank",rel:"noopener noreferrer",className:S,children:(0,O.jsxs)("div",{className:L,children:[(0,O.jsx)("h3",{className:P,children:a.name}),(0,O.jsx)("p",{className:D,children:a.desc}),(0,O.jsx)("div",{className:U,children:a.tags.map((e,a)=>(0,O.jsx)("span",{className:T,children:e},a))}),(0,O.jsxs)("div",{className:E,children:[e?"\u67e5\u770b\u6a21\u578b":"View Model"," ",(0,O.jsx)("span",{className:W,children:"\u2192"})]})]})},t))})]})})}function ie(){const e=(0,r.W)(),a=e?te:ae;return(0,O.jsx)("section",{id:"papers",className:F,children:(0,O.jsxs)("div",{className:c,children:[(0,O.jsxs)("div",{className:h,children:[(0,O.jsx)("h2",{className:m,children:e?"\u7cbe\u9009\u8bba\u6587":"Featured Papers"}),(0,O.jsx)("p",{className:g,children:e?"\u6211\u4eec\u56e2\u961f\u5728 RAG \u9886\u57df\u7684\u4ee3\u8868\u6027\u7814\u7a76\u5de5\u4f5c\u3002":"Representative research work from our team in the RAG domain."})]}),(0,O.jsx)("div",{className:I,children:a.map((a,t)=>(0,O.jsxs)("a",{href:a.href,target:a.href.startsWith("http")?"_blank":"_self",rel:"noopener noreferrer",className:Y,children:[(0,O.jsxs)("div",{className:z,children:[(0,O.jsx)("span",{className:X,children:a.date}),(0,O.jsx)("span",{className:H,children:a.venue})]}),(0,O.jsx)("h3",{className:V,children:a.title}),(0,O.jsx)("p",{className:Z,children:a.authors}),(0,O.jsx)("p",{className:K,children:a.abstract}),(0,O.jsxs)("div",{className:q,children:[e?"\u9605\u8bfb\u66f4\u591a":"Read More"," ",(0,O.jsx)("span",{className:B,children:"\u2192"})]})]},t))})]})})}function le(){const e=(0,r.W)();return(0,O.jsx)(n.A,{title:e?"\u7814\u7a76":"Research",description:"UltraRAG Research - Models and Papers",children:(0,O.jsxs)("main",{children:[(0,O.jsx)(se,{}),(0,O.jsx)(ne,{}),(0,O.jsx)(re,{}),(0,O.jsx)(ie,{})]})})}},7083(e,a,t){t.d(a,{W:()=>n});var s=t(6666);function n(){const{lang:e}=(0,s.o)();return"zh"===e}}}]);