<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">UltraRAG 2.1: Deep Knowledge Integration, Cross-Modal Support | UltraRAG</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://ultrarag.github.io/website/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://ultrarag.github.io/website/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://ultrarag.github.io/website/blog/ultrarag-2.1-release"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="UltraRAG 2.1: Deep Knowledge Integration, Cross-Modal Support | UltraRAG"><meta data-rh="true" name="description" content="In the process of building knowledge bases, setting up experimental systems, and evaluating results, researchers always encounter similar challenges: How to achieve multimodal retrieval and generation within a unified framework? How to efficiently integrate multi-source knowledge? And how to make complex RAG experiments easier to build and reproduce?"><meta data-rh="true" property="og:description" content="In the process of building knowledge bases, setting up experimental systems, and evaluating results, researchers always encounter similar challenges: How to achieve multimodal retrieval and generation within a unified framework? How to efficiently integrate multi-source knowledge? And how to make complex RAG experiments easier to build and reproduce?"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-11-11T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://mssssss123.github.io/,https://xinhaidong.top/"><meta data-rh="true" property="article:tag" content="release,ultrarag"><link data-rh="true" rel="icon" href="/website/img/logo.svg"><link data-rh="true" rel="canonical" href="https://ultrarag.github.io/website/blog/ultrarag-2.1-release"><link data-rh="true" rel="alternate" href="https://ultrarag.github.io/website/blog/ultrarag-2.1-release" hreflang="en"><link data-rh="true" rel="alternate" href="https://ultrarag.github.io/website/blog/ultrarag-2.1-release" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://ultrarag.github.io/website/blog/ultrarag-2.1-release","mainEntityOfPage":"https://ultrarag.github.io/website/blog/ultrarag-2.1-release","url":"https://ultrarag.github.io/website/blog/ultrarag-2.1-release","headline":"UltraRAG 2.1: Deep Knowledge Integration, Cross-Modal Support","name":"UltraRAG 2.1: Deep Knowledge Integration, Cross-Modal Support","description":"In the process of building knowledge bases, setting up experimental systems, and evaluating results, researchers always encounter similar challenges: How to achieve multimodal retrieval and generation within a unified framework? How to efficiently integrate multi-source knowledge? And how to make complex RAG experiments easier to build and reproduce?","datePublished":"2025-11-11T00:00:00.000Z","author":[{"@type":"Person","name":"Sen Mei","description":"TsinghuaNLP","url":"https://mssssss123.github.io/","image":"/website/img/team/ms.png"},{"@type":"Person","name":"Haidong Xin","description":"NEUIR","url":"https://xinhaidong.top/","image":"/website/img/team/xhd.jpg"}],"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://ultrarag.github.io/website/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/website/blog/rss.xml" title="UltraRAG RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/website/blog/atom.xml" title="UltraRAG Atom Feed"><link rel="stylesheet" href="/website/assets/css/styles.5d588865.css">
<script src="/website/assets/js/runtime~main.aa6eeb84.js" defer="defer"></script>
<script src="/website/assets/js/main.0b3925f5.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/website/img/_UltraRAG_logo.png"><link rel="preload" as="image" href="/website/img/team/ms.png"><link rel="preload" as="image" href="/website/img/team/xhd.jpg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/website/"><div class="navbar__logo"><img src="/website/img/_UltraRAG_logo.png" alt="UltraRAG Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/website/img/_UltraRAG_logo.png" alt="UltraRAG Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/website/">Home</a><div class="navbar__item megaMenuContainer_wtH9"><a class="navbar__link megaMenuTrigger_WfrV" href="/website/research">Research</a><div class="megaMenuDropdown_Gd7n"><div class="dropdownContent_kjhv"><div class="menuColumn_pPJS"><div class="columnTitle_O7QQ">Latest</div><ul class="menuList_YKg9"><li class="menuItem_7MPO"><a class="menuLink_SVdn" href="/website/blog">Blog</a></li></ul></div><div class="menuColumn_pPJS"><div class="columnTitle_O7QQ">Models</div><ul class="menuList_YKg9"><li class="menuItem_7MPO"><a href="https://huggingface.co/openbmb/AgentCPM-Report" target="_blank" rel="noopener noreferrer" class="menuLink_SVdn">AgentCPM-Report</a></li><li class="menuItem_7MPO"><a href="https://huggingface.co/openbmb/MiniCPM-Embedding-Light" target="_blank" rel="noopener noreferrer" class="menuLink_SVdn">MiniCPM-Embedding-Light</a></li></ul></div><div class="menuColumn_pPJS"><div class="columnTitle_O7QQ">Papers</div><ul class="menuList_YKg9"><li class="menuItem_7MPO"><a class="menuLink_SVdn" href="/website/research#papers">Featured Papers</a></li></ul></div></div></div></div><div class="navbar__item megaMenuContainer_wtH9"><a class="navbar__link megaMenuTrigger_WfrV" href="/website/team">Team</a><div class="megaMenuDropdown_Gd7n"><div class="dropdownContent_kjhv"><div class="menuColumn_pPJS"><div class="columnTitle_O7QQ">About</div><ul class="menuList_YKg9"><li class="menuItem_7MPO"><a class="menuLink_SVdn" href="/website/team">Members</a></li></ul></div><div class="menuColumn_pPJS"><div class="columnTitle_O7QQ">Connect</div><ul class="menuList_YKg9"><li class="menuItem_7MPO"><a class="menuLink_SVdn" href="/website/contact">Contact</a></li><li class="menuItem_7MPO"><a href="https://nlp.csai.tsinghua.edu.cn/job/29" target="_blank" rel="noopener noreferrer" class="menuLink_SVdn">Join Us</a></li></ul></div></div></div></div></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item"><a href="https://github.com/OpenBMB/UltraRAG" target="_blank" rel="noopener noreferrer" class="githubButton_sUw0" aria-label="Star OpenBMB/UltraRAG on GitHub"><div class="iconWrapper_DaS4"><svg height="24" width="24" viewBox="0 0 16 16" version="1.1" fill="currentColor" style="flex-shrink:0"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></div><div class="starText_BhhA">...</div></a></div><div class="switcher_Z8Ac"><button class="btn_xjeh active_YUPQ">EN</button><span class="divider_TN9w">|</span><button class="btn_xjeh">中文</button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2026</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/website/blog/ultrarag-3.0-release">UltraRAG 3.0: No More Black Boxes, Full Transparency in Reasoning</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/website/blog/ultrarag-2.1-release">UltraRAG 2.1: Deep Knowledge Integration, Cross-Modal Support</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/website/blog/ultrarag-2.0-release">UltraRAG 2.0: Minimal Code, Maximum Innovation</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">UltraRAG 2.1: Deep Knowledge Integration, Cross-Modal Support</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-11-11T00:00:00.000Z">November 11, 2025</time> · <!-- -->10 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://mssssss123.github.io/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="/website/img/team/ms.png" alt="Sen Mei"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://mssssss123.github.io/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp" translate="no">Sen Mei</span></a></div><small class="authorTitle_nd0D" title="TsinghuaNLP">TsinghuaNLP</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://xinhaidong.top/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="/website/img/team/xhd.jpg" alt="Haidong Xin"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://xinhaidong.top/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp" translate="no">Haidong Xin</span></a></div><small class="authorTitle_nd0D" title="NEUIR">NEUIR</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>In the process of building knowledge bases, setting up experimental systems, and evaluating results, researchers always encounter similar challenges: How to achieve multimodal retrieval and generation within a unified framework? How to efficiently integrate multi-source knowledge? And how to make complex RAG experiments easier to build and reproduce?</p><p><strong>UltraRAG 2.1</strong> addresses these research challenges with comprehensive upgrades focused on practical needs. This update brings core enhancements in three directions: <strong>native multimodal support, automated knowledge integration and corpus construction, and unified build-and-evaluate RAG workflows</strong>:</p><ul>
<li class=""><strong>Native Multimodal Support</strong>: Unified Retriever, Generation, and Evaluation modules with full multimodal retrieval and generation support; new <strong>VisRAG Pipeline</strong> enabling a complete closed-loop from local PDF indexing to multimodal retrieval and generation.</li>
<li class=""><strong>Automated Knowledge Integration &amp; Corpus Construction</strong>: Supports multi-format document parsing and chunked indexing, seamlessly integrating MinerU for easy construction of personalized knowledge bases.</li>
<li class=""><strong>Unified Build &amp; Evaluate RAG Workflows</strong>: Compatible with multiple retrieval and generation inference engines, providing a standardized evaluation system with full-chain visual analysis, achieving a unified process from model invocation to result verification.</li>
</ul><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="native-multimodal-support">Native Multimodal Support<a href="#native-multimodal-support" class="hash-link" aria-label="Direct link to Native Multimodal Support" title="Direct link to Native Multimodal Support" translate="no">​</a></h2><p>Previously, multimodal RAG often relied on multiple independent tools: text tasks and visual tasks belonged to different workflows, requiring researchers to switch between feature extraction, retrieval, generation, and evaluation tools, with inconsistent interfaces and difficult reproducibility.</p><p><strong>UltraRAG 2.1</strong> systematically integrates the multimodal RAG pipeline. All core Servers — <strong>Retriever, Generation, and Evaluation</strong> — now natively support multimodal tasks and can flexibly connect to various visual, text, or cross-modal models. Researchers can freely orchestrate their own multimodal pipelines within the unified framework — whether for document QA, image-text retrieval, or cross-modal generation — all achievable with minimal effort for end-to-end integration. Additionally, the framework&#x27;s built-in <strong>Benchmarks</strong> cover various tasks including visual QA, with a unified evaluation system for researchers to quickly conduct and compare multimodal experiments.</p><p>Building on this, <strong>UltraRAG 2.1 introduces the VisRAG Pipeline</strong>, enabling a complete closed-loop from local PDF indexing to multimodal retrieval and generation. This feature is based on the research in &quot;VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents,&quot; which proposes a vision-enhanced retrieval-augmented generation framework for multimodal documents. By jointly modeling document image information (such as charts, formulas, layout structures) with text content, it significantly improves content understanding and QA capabilities for complex scientific documents. UltraRAG integrates this approach, enabling researchers to reproduce VisRAG experiments directly on real PDF document scenarios and further extend multimodal retrieval-generation research and applications.</p><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="automated-knowledge-integration--corpus-construction">Automated Knowledge Integration &amp; Corpus Construction<a href="#automated-knowledge-integration--corpus-construction" class="hash-link" aria-label="Direct link to Automated Knowledge Integration &amp; Corpus Construction" title="Direct link to Automated Knowledge Integration &amp; Corpus Construction" translate="no">​</a></h2><p>During RAG development, developers need to repeatedly parse, clean, and chunk materials from different sources. As a result, the RAG construction process is often slowed by trivial engineering details, compressing the space for research innovation.</p><p><strong>UltraRAG 2.1&#x27;s</strong> <strong>Corpus Server</strong> makes all of this simple. Users can import corpora from different sources in one go without writing complex scripts — whether Word documents, e-books, or web archives — all automatically parsed into a unified text format. For PDF parsing, UltraRAG seamlessly integrates <strong>MinerU</strong>, accurately recognizing complex layouts and multi-column structures for high-fidelity text restoration. For mixed image-text files, it also supports converting PDFs page-by-page to images, making visual layouts part of the knowledge. For chunking strategies, <strong>Corpus Server</strong> offers multi-granularity options: supporting token-level, sentence-level, and custom rules, enabling fine-grained control of semantic boundaries while naturally adapting to structured text like Markdown.</p>
<!-- -->
<p><img decoding="async" loading="lazy" alt="UltraRAG 2.1 图示 1" src="/website/assets/images/1-3e19c1f17e8d186267edcb7ca36225f6.jpg" width="1280" height="657" class="img_ev3q"></p>
<p>Through this automated pipeline, Corpus Server modularizes the corpus import, parsing, and chunking process, reducing manual scripting and format adaptation work, enabling knowledge base construction to be directly integrated into the standardized RAG pipeline workflow.</p><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="unified-build--evaluate-rag-workflows">Unified Build &amp; Evaluate RAG Workflows<a href="#unified-build--evaluate-rag-workflows" class="hash-link" aria-label="Direct link to Unified Build &amp; Evaluate RAG Workflows" title="Direct link to Unified Build &amp; Evaluate RAG Workflows" translate="no">​</a></h2><blockquote>
<p>&quot;Chunking, indexing, retrieval, generation, evaluation — each step requires different scripts, too cumbersome!&quot;
&quot;Every time I change a parameter or switch a model, do I need to rebuild the entire pipeline?&quot;
&quot;After the experiment finally runs, how do I keep evaluation results consistent and comparable?&quot;</p>
</blockquote><p>These questions are frustrations that almost every RAG researcher has experienced. Existing frameworks often provide fragmented and incompatible support for retrieval, model integration, and evaluation, forcing researchers to repeatedly switch between different tools, with every modification potentially triggering a rebuild of the entire experimental chain. UltraRAG 2.1&#x27;s goal is to make complex workflows clear and unified again.</p><p>At the retrieval level, the framework supports sparse, dense, hybrid, and multimodal retrieval, compatible with multiple backend engines including Infinity, Sentence-Transformers, and OpenAI. Researchers can freely combine retrieval strategies and models for flexible pipeline design. For model generation, UltraRAG 2.1 simultaneously supports vLLM offline inference and Hugging Face local debugging, while maintaining full compatibility with the OpenAI interface, making model switching and deployment require no code changes. For evaluation, UltraRAG builds a unified Evaluation Server that can compute metrics like ACC and ROUGE for generated results, and supports TREC evaluation and significance analysis for retrieval results. Combined with the visual Case Study UI, researchers can intuitively compare the performance of different models and strategies, making &quot;debugging&quot; truly become &quot;understanding.&quot;</p><p>Furthermore, UltraRAG achieves full-chain integration from data import to retrieval, generation, and evaluation through a YAML configuration-driven workflow mechanism. Researchers only need to write minimal configuration files to quickly define and reproduce experimental workflows.</p>
<!-- -->
<p><img decoding="async" loading="lazy" alt="UltraRAG 2.1 图示 2" src="/website/assets/images/2-2c3fd8026c9eef51e9ec81ceb3ede664.jpg" width="1280" height="791" class="img_ev3q"></p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/website/blog/tags/release">release</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/website/blog/tags/ultrarag">ultrarag</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/website/blog/ultrarag-3.0-release"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">UltraRAG 3.0: No More Black Boxes, Full Transparency in Reasoning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/website/blog/ultrarag-2.0-release"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">UltraRAG 2.0: Minimal Code, Maximum Innovation</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#native-multimodal-support" class="table-of-contents__link toc-highlight">Native Multimodal Support</a></li><li><a href="#automated-knowledge-integration--corpus-construction" class="table-of-contents__link toc-highlight">Automated Knowledge Integration &amp; Corpus Construction</a></li><li><a href="#原生多模态支持" class="table-of-contents__link toc-highlight">原生多模态支持</a></li><li><a href="#知识接入与语料构建自动化" class="table-of-contents__link toc-highlight">知识接入与语料构建自动化</a></li><li><a href="#unified-build--evaluate-rag-workflows" class="table-of-contents__link toc-highlight">Unified Build &amp; Evaluate RAG Workflows</a></li><li><a href="#统一构建与评估的-rag-工作流" class="table-of-contents__link toc-highlight">统一构建与评估的 RAG 工作流</a></li></ul></div></div></div></div></div><footer class="theme-layout-footer footer"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 UltraRAG.</div></div></div></footer></div>
</body>
</html>