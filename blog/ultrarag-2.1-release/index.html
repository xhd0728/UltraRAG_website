<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">UltraRAG 2.1：纵深知识接入，横跨多模态支持 | UltraRAG</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://ultrarag.github.io/website/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://ultrarag.github.io/website/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://ultrarag.github.io/website/blog/ultrarag-2.1-release"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="UltraRAG 2.1：纵深知识接入，横跨多模态支持 | UltraRAG"><meta data-rh="true" name="description" content="在研究者构建知识库、搭建实验系统、评估实验结果的过程中，总会遇到相似的挑战：如何在一个统一框架中实现多模态检索与生成？如何高效接入多源知识？又如何让复杂的 RAG 实验更易搭建、更易复现？"><meta data-rh="true" property="og:description" content="在研究者构建知识库、搭建实验系统、评估实验结果的过程中，总会遇到相似的挑战：如何在一个统一框架中实现多模态检索与生成？如何高效接入多源知识？又如何让复杂的 RAG 实验更易搭建、更易复现？"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-11-11T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://mssssss123.github.io/,https://xinhaidong.top/"><meta data-rh="true" property="article:tag" content="release,ultrarag"><link data-rh="true" rel="icon" href="/website/img/logo.svg"><link data-rh="true" rel="canonical" href="https://ultrarag.github.io/website/blog/ultrarag-2.1-release"><link data-rh="true" rel="alternate" href="https://ultrarag.github.io/website/blog/ultrarag-2.1-release" hreflang="en"><link data-rh="true" rel="alternate" href="https://ultrarag.github.io/website/blog/ultrarag-2.1-release" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://ultrarag.github.io/website/blog/ultrarag-2.1-release","mainEntityOfPage":"https://ultrarag.github.io/website/blog/ultrarag-2.1-release","url":"https://ultrarag.github.io/website/blog/ultrarag-2.1-release","headline":"UltraRAG 2.1：纵深知识接入，横跨多模态支持","name":"UltraRAG 2.1：纵深知识接入，横跨多模态支持","description":"在研究者构建知识库、搭建实验系统、评估实验结果的过程中，总会遇到相似的挑战：如何在一个统一框架中实现多模态检索与生成？如何高效接入多源知识？又如何让复杂的 RAG 实验更易搭建、更易复现？","datePublished":"2025-11-11T00:00:00.000Z","author":[{"@type":"Person","name":"Sen Mei","description":"TsinghuaNLP","url":"https://mssssss123.github.io/","image":"/website/img/team/ms.png"},{"@type":"Person","name":"Haidong Xin","description":"NEUIR","url":"https://xinhaidong.top/","image":"/website/img/team/xhd.jpg"}],"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://ultrarag.github.io/website/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/website/blog/rss.xml" title="UltraRAG RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/website/blog/atom.xml" title="UltraRAG Atom Feed"><link rel="stylesheet" href="/website/assets/css/styles.5a5d82c2.css">
<script src="/website/assets/js/runtime~main.746466ac.js" defer="defer"></script>
<script src="/website/assets/js/main.04e6784b.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/website/img/logo.svg"><link rel="preload" as="image" href="/website/img/team/ms.png"><link rel="preload" as="image" href="/website/img/team/xhd.jpg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/website/"><div class="navbar__logo"><img src="/website/img/logo.svg" alt="UltraRAG Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/website/img/logo.svg" alt="UltraRAG Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><div class="navbar__item megaMenuContainer_wtH9"><div class="navbar__link megaMenuTrigger_WfrV" role="button" tabindex="0">文档</div><div class="megaMenuDropdown_Gd7n"><div class="dropdownContent_kjhv"><div class="menuColumn_pPJS"><div class="columnTitle_O7QQ">教程</div><ul class="menuList_YKg9"><li class="menuItem_7MPO"><a href="https://ultrarag.openbmb.cn/" target="_blank" rel="noopener noreferrer" class="menuLink_SVdn">主页</a></li><li class="menuItem_7MPO"><a href="https://ultrarag.openbmb.cn/pages/cn/getting_started/installation" target="_blank" rel="noopener noreferrer" class="menuLink_SVdn">安装</a></li><li class="menuItem_7MPO"><a href="https://ultrarag.openbmb.cn/pages/cn/getting_started/quick_start" target="_blank" rel="noopener noreferrer" class="menuLink_SVdn">运行基准实验</a></li><li class="menuItem_7MPO"><a href="https://ultrarag.openbmb.cn/pages/cn/ui/start" target="_blank" rel="noopener noreferrer" class="menuLink_SVdn">启动交互界面</a></li></ul></div><div class="menuColumn_pPJS"><div class="columnTitle_O7QQ">最新动态</div><ul class="menuList_YKg9"><li class="menuItem_7MPO"><a class="menuLink_SVdn" href="/website/blog">博客</a></li></ul></div></div></div></div><div class="navbar__item megaMenuContainer_wtH9"><div class="navbar__link megaMenuTrigger_WfrV" role="button" tabindex="0">项目</div><div class="megaMenuDropdown_Gd7n"><div class="dropdownContent_kjhv"><div class="menuColumn_pPJS"><div class="columnTitle_O7QQ">UltraRAG</div><ul class="menuList_YKg9"><li class="menuItem_7MPO"><a class="menuLink_SVdn" href="/website/daily-papers">试玩</a></li><li class="menuItem_7MPO"><a href="https://modelscope.cn/datasets/UltraRAG/UltraRAG_Benchmark" target="_blank" rel="noopener noreferrer" class="menuLink_SVdn">数据集 &amp; 语料库</a></li></ul></div><div class="menuColumn_pPJS"><div class="columnTitle_O7QQ">模型</div><ul class="menuList_YKg9"><li class="menuItem_7MPO"><a href="https://huggingface.co/openbmb/AgentCPM-Report" target="_blank" rel="noopener noreferrer" class="menuLink_SVdn">AgentCPM-Report</a></li><li class="menuItem_7MPO"><a href="https://huggingface.co/openbmb/MiniCPM-Embedding-Light" target="_blank" rel="noopener noreferrer" class="menuLink_SVdn">MiniCPM-Embedding-Light</a></li></ul></div><div class="menuColumn_pPJS"><div class="columnTitle_O7QQ">研究</div><ul class="menuList_YKg9"><li class="menuItem_7MPO"><a class="menuLink_SVdn" href="/website/daily-papers">每日论文</a></li></ul></div></div></div></div><div class="navbar__item megaMenuContainer_wtH9"><div class="navbar__link megaMenuTrigger_WfrV" role="button" tabindex="0">团队</div><div class="megaMenuDropdown_Gd7n"><div class="dropdownContent_kjhv"><div class="menuColumn_pPJS"><div class="columnTitle_O7QQ">关于我们</div><ul class="menuList_YKg9"><li class="menuItem_7MPO"><a class="menuLink_SVdn" href="/website/team">团队成员</a></li></ul></div><div class="menuColumn_pPJS"><div class="columnTitle_O7QQ">联系</div><ul class="menuList_YKg9"><li class="menuItem_7MPO"><a class="menuLink_SVdn" href="/website/contact">联系我们</a></li><li class="menuItem_7MPO"><a href="https://nlp.csai.tsinghua.edu.cn/job/29" target="_blank" rel="noopener noreferrer" class="menuLink_SVdn">加入我们</a></li></ul></div></div></div></div><div class="navbar__item"><a href="https://github.com/OpenBMB/UltraRAG" target="_blank" rel="noopener noreferrer" class="githubButton_sUw0" aria-label="Star OpenBMB/UltraRAG on GitHub"><div class="iconWrapper_DaS4"><svg height="24" width="24" viewBox="0 0 16 16" version="1.1" fill="currentColor" style="flex-shrink:0"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></div><div class="starText_BhhA">...</div></a></div></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2026</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/website/blog/ultrarag-3.0-release">UltraRAG 3.0：告别黑盒，推理逻辑全透明</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/website/blog/ultrarag-2.1-release">UltraRAG 2.1：纵深知识接入，横跨多模态支持</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/website/blog/ultrarag-2.0-release">UltraRAG 2.0：代码极简化，创新最大化</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">UltraRAG 2.1：纵深知识接入，横跨多模态支持</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-11-11T00:00:00.000Z">November 11, 2025</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://mssssss123.github.io/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="/website/img/team/ms.png" alt="Sen Mei"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://mssssss123.github.io/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp" translate="no">Sen Mei</span></a></div><small class="authorTitle_nd0D" title="TsinghuaNLP">TsinghuaNLP</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://xinhaidong.top/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="/website/img/team/xhd.jpg" alt="Haidong Xin"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://xinhaidong.top/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp" translate="no">Haidong Xin</span></a></div><small class="authorTitle_nd0D" title="NEUIR">NEUIR</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>在研究者构建知识库、搭建实验系统、评估实验结果的过程中，总会遇到相似的挑战：如何在一个统一框架中实现多模态检索与生成？如何高效接入多源知识？又如何让复杂的 RAG 实验更易搭建、更易复现？</p>
<p><strong>UltraRAG 2.1</strong> 在这些科研挑战的背景下，进行了面向实际研究需求的全面升级。本次更新围绕 <strong>原生多模态支持、知识接入与语料构建自动化、统一构建与评估的 RAG 工作流</strong> 三大方向带来了核心增强：</p>
<ul>
<li class=""><strong>原生多模态支持</strong>：统一 Retriever、Generation 与 Evaluation 模块，全面支持多模态检索与生成；新增 <strong>VisRAG Pipeline</strong>，实现从本地 PDF 建库到多模态检索与生成的完整闭环。</li>
<li class=""><strong>知识接入与语料构建自动化</strong>：支持多格式文档解析与分块建库，无缝集成 MinerU，轻松构建个人化知识库。</li>
<li class=""><strong>统一构建与评估的 RAG 工作流</strong>：适配多种检索与生成推理引擎，提供标准化的评估体系，支持全链路可视化分析，实现从模型调用到结果验证的统一流程。</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="原生多模态支持">原生多模态支持<a href="#原生多模态支持" class="hash-link" aria-label="Direct link to 原生多模态支持" title="Direct link to 原生多模态支持" translate="no">​</a></h2>
<p>过去，多模态 RAG 往往需要依赖多套独立工具：文本任务与视觉任务分属不同流程，研究者需在特征提取、检索、生成和评估工具间来回切换，接口不统一、复现困难。</p>
<p><strong>UltraRAG 2.1</strong> 对多模态 RAG 流程进行了系统化整合。所有核心 Server——<strong>Retriever、Generation 与 Evaluation</strong>——均已原生支持多模态任务，可灵活接入各种视觉、文本、或跨模态模型。研究者可在统一框架内自由编排属于自己的多模态 pipeline，无论是文档问答、图文检索，还是跨模态生成，都能以最小代价实现端到端联通。此外，框架内置的 <strong>Benchmark</strong> 覆盖视觉问答等多种任务，并提供统一的评估体系，方便研究者快速开展和对比多模态实验。</p>
<p>在此基础上，<strong>UltraRAG 2.1 引入 VisRAG Pipeline</strong>，实现从本地 PDF 建库到多模态检索与生成的完整闭环。该功能基于论文《VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents》的研究成果，论文提出了一个面向多模态文档的视觉增强检索生成框架，通过将文档图像信息（如图表、公式、版面结构）与文本内容联合建模，显著提升了模型在复杂科学文档中的内容理解与问答能力。UltraRAG 将这一方法集成，使研究者能够直接在真实 PDF 文档场景中复现 VisRAG 的实验过程，并进一步扩展多模态检索生成的研究与应用。</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="知识接入与语料构建自动化">知识接入与语料构建自动化<a href="#知识接入与语料构建自动化" class="hash-link" aria-label="Direct link to 知识接入与语料构建自动化" title="Direct link to 知识接入与语料构建自动化" translate="no">​</a></h2>
<p>在 RAG 开发过程中，面对不同来源的资料，开发者需要反复解析、清洗、分块。结果是，RAG 的构建过程往往被琐碎的工程细节拖慢，科研创新的空间反而被压缩。</p>
<p><strong>UltraRAG 2.1</strong> 的 <strong>Corpus Server</strong> 让这一切变得简单。用户无需编写复杂脚本，就能一次性导入来自不同来源的语料——无论是 word 文档还是电子书与网页存档，都能被自动解析为统一的文本格式。在 PDF 解析方面，UltraRAG 无缝集成 <strong>MinerU</strong>，能够精确识别复杂版面与多栏结构，实现高保真文本还原。对于图文混排文件，还支持将 PDF 按页转换为图像，让视觉布局也能成为知识的一部分。在分块策略上，<strong>Corpus Server</strong> 提供了多粒度选择：支持词元级、句子级与自定义规则，既能精细控制语义边界，又能自然适配 Markdown 等结构化文本。</p>
<p><img decoding="async" loading="lazy" alt="UltraRAG 2.1 图示 1" src="/website/assets/images/1-3e19c1f17e8d186267edcb7ca36225f6.jpg" width="1280" height="657" class="img_ev3q"></p>
<p>通过这一整套自动化流程，Corpus Server 将语料导入、解析与分块过程模块化，减少了手工脚本与格式适配工作，使知识库构建可以直接融入 RAG pipeline 的标准化流程中。</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="统一构建与评估的-rag-工作流">统一构建与评估的 RAG 工作流<a href="#统一构建与评估的-rag-工作流" class="hash-link" aria-label="Direct link to 统一构建与评估的 RAG 工作流" title="Direct link to 统一构建与评估的 RAG 工作流" translate="no">​</a></h2>
<blockquote>
<p>“切块、索引、检索、生成、评估，每一步都要用不同的脚本，太繁琐了！”
“每改一次参数、换一个模型，是否又要重搭整条 pipeline？”
“当实验终于跑通后，评估结果又该怎样保持一致与可比？”</p>
</blockquote>
<p>这些问题几乎是每个 RAG 研究者都经历过的烦恼。现有框架对检索、模型接入、评估的支持往往零散且不兼容，研究者不得不在不同工具之间反复切换，每一次修改都可能引发整条实验链路的重建。UltraRAG 2.1 的目标，就是让复杂的流程重新变得清晰而统一。</p>
<p>在检索层面，框架支持稀疏、稠密、混合与多模态检索，并兼容 Infinity、Sentence-Transformers、OpenAI 等多种后端引擎，研究者可以自由组合检索策略与模型，实现灵活的 pipeline 设计。在模型生成部分，UltraRAG 2.1 同时支持 vLLM 离线推理 与 Hugging Face 本地调试，并保持与 OpenAI 接口 完全兼容，使模型切换与部署无需修改代码。在评估环节，UltraRAG 构建了统一的 Evaluation Server，既能对生成结果计算 ACC、ROUGE 等指标，又支持对检索结果进行 TREC 评估与显著性分析。配合可视化的 Case Study UI，研究者可以直观地比较不同模型与策略的表现，让“调试”真正变成“理解”。</p>
<p>此外，UltraRAG 通过 YAML 配置驱动的工作流机制，实现了从数据导入到检索、生成与评估的全链路串联，研究者只需编写少量配置文件，即可快速定义和复现实验流程。</p>
<p><img decoding="async" loading="lazy" alt="UltraRAG 2.1 图示 2" src="/website/assets/images/2-2c3fd8026c9eef51e9ec81ceb3ede664.jpg" width="1280" height="791" class="img_ev3q"></p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/website/blog/tags/release">release</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/website/blog/tags/ultrarag">ultrarag</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/website/blog/ultrarag-3.0-release"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">UltraRAG 3.0：告别黑盒，推理逻辑全透明</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/website/blog/ultrarag-2.0-release"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">UltraRAG 2.0：代码极简化，创新最大化</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#原生多模态支持" class="table-of-contents__link toc-highlight">原生多模态支持</a></li><li><a href="#知识接入与语料构建自动化" class="table-of-contents__link toc-highlight">知识接入与语料构建自动化</a></li><li><a href="#统一构建与评估的-rag-工作流" class="table-of-contents__link toc-highlight">统一构建与评估的 RAG 工作流</a></li></ul></div></div></div></div></div><footer class="theme-layout-footer footer"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 UltraRAG.</div></div></div></footer></div>
</body>
</html>